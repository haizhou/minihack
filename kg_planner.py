"""
kg_planner.py â€” Knowledge-graph path planner + dynamic state tracker
=====================================================================
KGPathState: æ ¸å¿ƒæ”¹è¿›
  - æ¯ä¸ª episode ç‹¬ç«‹è·Ÿè¸ª agent åœ¨ KG è·¯å¾„ä¸Šçš„å½“å‰ä½ç½®
  - bias å‘é‡éšè·¯å¾„æ¨è¿›åŠ¨æ€ç¼©å‡ï¼ˆåªå‰©ä½™è·¯å¾„èŠ‚ç‚¹è·å¾— boostï¼‰
  - intrinsic reward = KG è¾¹ç©¿è¶Šæ—¶ç›®æ ‡èŠ‚ç‚¹çš„æ¦‚ç‡ï¼ˆæ¥è‡ª world knowledgeï¼Œéäººå·¥è®¾ç½®ï¼‰
  - è¾¹ç©¿è¶Šæ£€æµ‹: åŸºäº edge relation è¯­ä¹‰ + NODE_TO_CHAR glyph æ˜ å°„
"""
import json
import os
import numpy as np
import torch
from collections import deque


# â”€â”€ KG èŠ‚ç‚¹ â†’ å¯¹åº” glyph å­—ç¬¦é›†ï¼ˆè¿æ¥ç¬¦å· KG ä¸è§†è§‰è§‚æµ‹çš„æ¡¥æ¢ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
NODE_TO_CHAR = {
    'key':    {ord('(')},
    'door':   {ord('+')},
    'open':   {ord('.')},
    'stairs': {ord('>')},
}

# Option å…³é”®è¯ â†’ è´Ÿè´£åˆ°è¾¾çš„ KG èŠ‚ç‚¹ï¼ˆoption å‘½åçº¦å®šçš„è¯­ä¹‰æ˜ å°„ï¼‰
OPTION_TO_NODE = {
    'findkey':    'key',
    'find':       'key',
    'pickup':     'key',
    'opendoor':   'door',
    'open':       'door',
    'gotostairs': 'stairs',
    'stair':      'stairs',
}

# KG edge relation â†’ ç©¿è¶Šè¯¥è¾¹æ—¶ä½¿ç”¨çš„è§‚æµ‹æ£€æµ‹æ–¹å¼
# è¯­ä¹‰æ¥è‡ª edge relation æœ¬èº«çš„å«ä¹‰ï¼Œä¸ä¾èµ–ç‰¹å®šç¯å¢ƒè§„åˆ™ï¼š
#   can_pickup:   ç‰©å“ä»åœ°å›¾æ¶ˆå¤±ï¼ˆè¢«æ‹¾å–ï¼‰â†’ glyph æ•°é‡å‡å°‘
#   enables:      æŒæœ‰ç‰©å“åè§¦å‘çŠ¶æ€å˜åŒ– â†’ ç›®æ ‡ glyph æ•°é‡å‡å°‘
#   state_change: å®ä½“çŠ¶æ€æ”¹å˜         â†’ ç›®æ ‡ glyph æ•°é‡å‡å°‘
EDGE_DETECTION = {
    'can_pickup':   'glyph_decrease',
    'enables':      'glyph_decrease',
    'state_change': 'glyph_decrease',
}


def load_graph(path='data/knowledge_graph.json'):
    with open(path) as f:
        return json.load(f)['graph']


def bfs_full_path(graph, start, goal):
    queue   = deque([(start, [])])
    visited = {start}
    while queue:
        node, path = queue.popleft()
        if node == goal:
            return path
        for e in graph.get(node, []):
            if e['target'] not in visited:
                visited.add(e['target'])
                queue.append((e['target'], path + [(node, e['relation'], e['target'])]))
    return []


def get_node_probs(graph, start, goal, n_other_estimate=138):
    path = bfs_full_path(graph, start, goal)
    if not path:
        return {}, []
    path_nodes  = [t for s, r, t in path if t != goal]
    n_path      = len(path_nodes)
    path_scores = torch.tensor([1.0 / (i + 1) for i in range(n_path)])
    path_probs  = path_scores / path_scores.sum() * 0.9
    probs = {node: p for node, p in zip(path_nodes, path_probs.tolist())}
    return probs, path


def _bias_for_path_segment(remaining_path, node_probs, options, bias_scale=3.0, path_decay=0.6):
    """
    ç»™å®šå‰©ä½™è·¯å¾„ç‰‡æ®µï¼Œè®¡ç®— option bias å‘é‡ã€‚
    åªæœ‰å‰©ä½™è·¯å¾„ä¸­çš„ç›®æ ‡èŠ‚ç‚¹å¯¹åº”çš„ option è·å¾—æ­£å‘ biasï¼Œ
    å·²å®ŒæˆèŠ‚ç‚¹è‡ªåŠ¨é€€å‡ºï¼Œä¸éœ€è¦ä»»ä½•æ‰‹åŠ¨å±è”½ã€‚
    """
    n      = len(options)
    bias   = torch.zeros(n)
    seen   = set()

    for step_idx, (src, rel, dst) in enumerate(remaining_path):
        if dst == 'open':        # ç»ˆç‚¹ä¸å¯¹åº”å…·ä½“ option
            continue
        decay    = path_decay ** step_idx
        node_p   = node_probs.get(dst, 0.0)
        for i, opt in enumerate(options):
            name = opt.name.lower()
            matched_node = next(
                (nd for kw, nd in OPTION_TO_NODE.items() if kw in name and nd == dst),
                None
            )
            if matched_node and (i, dst) not in seen:
                bias[i] += node_p * bias_scale * decay
                seen.add((i, dst))

    for i, opt in enumerate(options):
        if 'explore' in opt.name.lower():
            bias[i] -= 0.5

    return bias


def get_option_weights(graph, options, env_name, start='agent', goal='open'):
    """åˆå§‹é™æ€æƒé‡ï¼šè®­ç»ƒå¼€å§‹å‰ç”¨äºæ‰“å°ï¼Œå®é™… bias ç”± KGPathState åŠ¨æ€ç®¡ç†ã€‚"""
    if 'KeyRoom' not in env_name:
        return torch.zeros(len(options))

    node_probs, path = get_node_probs(graph, start, goal)

    print(f"[KG] Optimal path:")
    for s, r, t in path:
        print(f"[KG]   {s:20s} -[{r}]-> {t}")
    print(f"[KG] Node probabilities:")
    for node, p in node_probs.items():
        print(f"[KG]   {node:20s}: p={p:.4f}")

    weights = _bias_for_path_segment(path, node_probs, options)
    return weights


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# KGPathState â€” æ ¸å¿ƒæ–°å¢ï¼šæ¯ä¸ª episode çš„ KG è·¯å¾„çŠ¶æ€è¿½è¸ªå™¨
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class KGPathState:
    """
    åŠ¨æ€è·Ÿè¸ª agent åœ¨ KG è·¯å¾„ä¸Šçš„å½“å‰ä½ç½®ï¼Œå¹¶æ®æ­¤æä¾›ï¼š
      1. åŠ¨æ€ option bias å‘é‡ï¼ˆä»…å‰©ä½™å­ç›®æ ‡è·å¾— boostï¼‰
      2. KG-derived intrinsic rewardï¼ˆè¾¹ç©¿è¶Šæ—¶é‡Šæ”¾ï¼Œå¤§å° = ç›®æ ‡èŠ‚ç‚¹æ¦‚ç‡ * etaï¼‰

    è¾¹ç©¿è¶Šæ£€æµ‹ç­–ç•¥ï¼ˆæ¥è‡ª edge relation è¯­ä¹‰ï¼Œä¸ä¾èµ–ç¯å¢ƒç‰¹å®šè§„åˆ™ï¼‰ï¼š
      - 'glyph_decrease': ç›®æ ‡èŠ‚ç‚¹å¯¹åº”çš„ glyph å­—ç¬¦åœ¨åœ°å›¾ä¸Šçš„æ•°é‡å‡å°‘
        ï¼ˆè¦†ç›– can_pickup / enables / state_change ä¸‰ç§ relationï¼‰

    æ‰€æœ‰æ¦‚ç‡å€¼æ¥è‡ª world_knowledge å­¦ä¹ åˆ°çš„ KGï¼Œeta æ˜¯å”¯ä¸€è¶…å‚æ•°ã€‚
    """

    def __init__(self, path, node_probs, options, eta=0.5, self_update_lr=0.01):
        """
        path:            [(src, relation, dst), ...] from bfs_full_path
        node_probs:      {node: float} from get_node_probs  (learned from world knowledge)
        options:         list of Option objects
        eta:             intrinsic reward scaling factor
        self_update_lr:  EMA learning rate for self-updating node probs from experience
        """
        self.path            = path
        self.node_probs      = dict(node_probs)   # mutable copy for self-update
        self.options         = options
        self.eta             = eta
        self.self_update_lr  = self_update_lr

        # é¢„è®¡ç®—æ¯ä¸ªè·¯å¾„ä½ç½®å¯¹åº”çš„ bias å‘é‡ï¼ˆä½ç½® i = å‰ i æ¡è¾¹å·²å®Œæˆï¼‰
        self._bias_cache = [
            _bias_for_path_segment(path[i:], node_probs, options)
            for i in range(len(path) + 1)
        ]

        self.current_step = 0   # å½“å‰åœ¨ path ä¸­çš„ç´¢å¼•
        self._prev_chars  = None
        self._last_key_obs_pos = None   # æœ¬ episode æœ€åä¸€æ¬¡çœ‹åˆ° key çš„åœ°å›¾åæ ‡ï¼ˆé®æŒ¡ä¿®å¤ç”¨ï¼‰

        # episode å†…æ›¾å‡ºç°è¿‡çš„èŠ‚ç‚¹é›†åˆï¼ˆç”¨äº discoverï¼šé¿å…è¢« @ é®æŒ¡å¯¼è‡´æ‰«æå¤±è´¥ï¼‰
        self._nodes_seen_this_episode: set = set()

        # â”€â”€ Self-update state â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        self._episode_traversed = []   # edge indices traversed this episode
        self._total_episodes    = 0
        self._update_interval   = 20   # recompute bias cache every N episodes

        # â”€â”€ Ordering constraint learning â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Agent discovers dependencies by trying options prematurely and failing.
        # _premature_calls[node] = how many times that option was called before
        #   its prerequisite sub-goal was reached.
        # _ordering_constraints[node] = prerequisite node that must come first.
        # _max_premature: threshold of failures before constraint is registered.
        self._premature_calls       = {}   # {node: int}
        self._ordering_constraints  = {}   # {node: prerequisite_node}
        self._max_premature         = 15   # fast enough to learn, stable enough to trust

    def reset(self, init_obs):
        """æ¯ä¸ª episode å¼€å§‹æ—¶é‡ç½®çŠ¶æ€ã€‚"""
        self.current_step = 0
        self._prev_chars  = init_obs.get('chars')
        self._last_key_obs_pos = None
        self._nodes_seen_this_episode = set()
        # æ‰«æåˆå§‹è§‚æµ‹ï¼šè®°å½•åˆå§‹å¯è§èŠ‚ç‚¹ + å®šä½ key åˆå§‹åæ ‡
        chars = init_obs.get('chars')
        if chars is not None:
            key_char_set = NODE_TO_CHAR.get('key', set())
            for rr in range(chars.shape[0]):
                for cc in range(chars.shape[1]):
                    if chars[rr, cc] in key_char_set:
                        self._last_key_obs_pos = (rr, cc)
                        break
            for node_name, char_set in NODE_TO_CHAR.items():
                if any(int(np.sum(chars == c)) > 0 for c in char_set):
                    self._nodes_seen_this_episode.add(node_name)

    def update(self, pre_obs, post_obs, option_name=''):
        """
        option æ‰§è¡Œå®Œåè°ƒç”¨ã€‚æ¯”è¾ƒæ‰§è¡Œå‰åçš„è§‚æµ‹ï¼Œåˆ¤æ–­æ˜¯å¦ç©¿è¶Šäº†ä¸‹ä¸€æ¡ KG è¾¹ã€‚

        è¿”å›: intrinsic_reward (float)
          - ç©¿è¶Šäº†è¾¹: = node_probs[dst] * eta
          - æœªç©¿è¶Š:   = 0.0
        """
        # æŒç»­è®°å½• episode å†…è§è¿‡çš„æ‰€æœ‰èŠ‚ç‚¹ï¼ˆç”¨äº discover_from_victory ç¿»æ—§è´¦ï¼‰
        pre_chars_scan = pre_obs.get('chars')
        if pre_chars_scan is not None:
            for _node, _cset in NODE_TO_CHAR.items():
                if any(int(np.sum(pre_chars_scan == c)) > 0 for c in _cset):
                    self._nodes_seen_this_episode.add(_node)

        if self.current_step >= len(self.path):
            self._prev_chars = post_obs.get('chars')
            return 0.0

        src, rel, dst = self.path[self.current_step]
        detection     = EDGE_DETECTION.get(rel, 'glyph_decrease')
        traversed     = False

        # æ›´æ–° key æœ€åå¯è§åæ ‡ï¼ˆåœ¨åšæ£€æµ‹å‰å…ˆè®°å½•ï¼Œä»¥ä¾¿é®æŒ¡å…œåº•ä½¿ç”¨ï¼‰
        if dst == 'key':
            pre_chars_arr = pre_obs.get('chars')
            if pre_chars_arr is not None:
                key_char_set = NODE_TO_CHAR.get('key', set())
                for rr in range(pre_chars_arr.shape[0]):
                    for cc in range(pre_chars_arr.shape[1]):
                        if pre_chars_arr[rr, cc] in key_char_set:
                            self._last_key_obs_pos = (rr, cc)
                            break

        if detection == 'glyph_decrease':
            traversed = self._glyph_decreased(pre_obs, post_obs, dst)

        # é®æŒ¡å…œåº•ï¼šagent ç«™åœ¨ key ä¸Šæ—¶ pre_count/post_count å‡ä¸º 0ï¼Œå¯¼è‡´ glyph_decrease æ¼åˆ¤
        # æ¡ä»¶ï¼š1) ä¸»æ£€æµ‹æœªé€šè¿‡  2) ç›®æ ‡èŠ‚ç‚¹æ˜¯ key  3) æ‰§è¡Œçš„æ˜¯æ‹¾å–ç±» option
        #       4) æœ¬ episode æ›¾è§è¿‡ keyï¼ˆ_last_key_obs_pos ä¸ä¸º Noneï¼‰
        #       5) option ç»“æŸå key å·²æ¶ˆå¤±
        if not traversed and dst == 'key' and self._last_key_obs_pos is not None:
            is_pickup = any(kw in option_name.lower() for kw in ('findkey', 'pickup'))
            if is_pickup:
                pre_blstats = pre_obs.get('blstats')
                if pre_blstats is not None:
                    agent_pos = (int(pre_blstats[1]), int(pre_blstats[0]))
                    if agent_pos == self._last_key_obs_pos:
                        post_chars_arr = post_obs.get('chars')
                        if post_chars_arr is not None:
                            post_key_count = sum(
                                int(np.sum(post_chars_arr == c))
                                for c in NODE_TO_CHAR.get('key', set())
                            )
                            if post_key_count == 0:
                                traversed = True

        intr_r = 0.0
        if traversed:
            self._episode_traversed.append(self.current_step)   # record before increment
            self.current_step += 1
            intr_r = self.node_probs.get(dst, 0.0) * self.eta
            if intr_r > 0:
                print(f"  [KGâœ“] Edge traversed: {src} -[{rel}]-> {dst}  "
                      f"| step={self.current_step}/{len(self.path)}"
                      f"  intr_r={intr_r:.4f}")

        self._prev_chars = post_obs.get('chars')
        return intr_r

    def get_bias(self, kg_decay=1.0):
        """
        è¿”å›å½“å‰è·¯å¾„ä½ç½®çš„åŠ¨æ€ bias å‘é‡ï¼Œä¹˜ä»¥ kg_decayã€‚
        å·²å®Œæˆçš„å­ç›®æ ‡èŠ‚ç‚¹ä¸å†å‡ºç°åœ¨ bias ä¸­ã€‚
        å åŠ å·²å­¦åˆ°çš„é¡ºåºçº¦æŸæƒ©ç½šï¼ˆprerequisite æœªå®Œæˆæ—¶å‹åˆ¶è¢«ä¾èµ– optionï¼‰ã€‚
        """
        idx       = min(self.current_step, len(self._bias_cache) - 1)
        base_bias = self._bias_cache[idx] * kg_decay

        # Apply learned ordering constraints
        if self._ordering_constraints and kg_decay > 0:
            penalty = torch.zeros_like(base_bias)
            for i, opt in enumerate(self.options):
                opt_node = next(
                    (nd for kw, nd in OPTION_TO_NODE.items() if kw in opt.name.lower()),
                    None,
                )
                if opt_node not in self._ordering_constraints:
                    continue
                prereq_node = self._ordering_constraints[opt_node]
                # Find the path step at which prereq_node is reached
                prereq_step = next(
                    (j for j, (_, _, t) in enumerate(self.path) if t == prereq_node),
                    None,
                )
                if prereq_step is not None and self.current_step <= prereq_step:
                    # Prerequisite not yet achieved â†’ penalise this option
                    penalty[i] -= 2.5
            base_bias = base_bias + penalty * kg_decay

        return base_bias

    def progress(self):
        """è¿”å› (current_step, total_steps) ç”¨äºæ—¥å¿—ã€‚"""
        return self.current_step, len(self.path)

    def record_option_attempt(self, option_name: str, pre_obs, post_obs):
        """
        æ¯æ¬¡ option æ‰§è¡Œå®Œã€update() æ¨è¿›è·¯å¾„æ­¥éª¤ä¹‹å‰è°ƒç”¨ã€‚

        æ£€æµ‹"è¶…å‰è°ƒç”¨"ï¼šè¯¥ option å¯¹åº”çš„ KG èŠ‚ç‚¹ä¸æ˜¯å½“å‰ä¸‹ä¸€æ­¥ï¼Œè€Œæ˜¯æœªæ¥æŸæ­¥ã€‚
        æ¯æ¬¡è¶…å‰è°ƒç”¨è®¡æ•° +1ï¼›è¾¾åˆ° _max_premature é˜ˆå€¼åï¼Œ
        è‡ªåŠ¨æ³¨å†Œ ordering constraintï¼ˆè¯¥èŠ‚ç‚¹éœ€è¦å½“å‰ä¸‹ä¸€æ­¥å…ˆå®Œæˆï¼‰ã€‚

        è¿™æ˜¯ agent æ— éœ€å¤–éƒ¨æŒ‡å¯¼è‡ªè¡Œå‘ç° option ä¾èµ–å…³ç³»çš„æœºåˆ¶ã€‚
        """
        opt_node = next(
            (nd for kw, nd in OPTION_TO_NODE.items() if kw in option_name.lower()),
            None,
        )
        if opt_node is None or self.current_step >= len(self.path):
            return

        _, _, current_next = self.path[self.current_step]
        if opt_node == current_next:
            return  # æ­£ç¡®é¡ºåºï¼Œä¸è®°å½•

        # æ˜¯å¦æ˜¯è·¯å¾„ä¸­é åçš„èŠ‚ç‚¹ï¼Ÿ
        future_nodes = [t for _, _, t in self.path[self.current_step + 1:]]
        if opt_node not in future_nodes:
            return  # ä¸åœ¨è·¯å¾„ä¸­ï¼Œå¿½ç•¥

        # è¶…å‰è°ƒç”¨ï¼šè®¡æ•°
        self._premature_calls[opt_node] = self._premature_calls.get(opt_node, 0) + 1
        count = self._premature_calls[opt_node]

        if count == self._max_premature:
            # é˜ˆå€¼è§¦å‘ï¼šæ³¨å†Œ ordering constraint
            self._ordering_constraints[opt_node] = current_next
            print(
                f"  [KGğŸ§ ] Learned dependency: '{opt_node}' requires '{current_next}' first"
                f"  (after {count} premature attempts)"
            )
            # get_bias() ä¼šåœ¨ä¸‹æ¬¡è°ƒç”¨æ—¶è‡ªåŠ¨åº”ç”¨æ­¤çº¦æŸï¼Œæ— éœ€é‡ç®— cache

    def discover_from_victory(self, victory_obs, pre_victory_obs=None):
        """
        å‘ç°æ¨¡å¼ï¼šagent èƒœåˆ©æ—¶è°ƒç”¨ã€‚

        æ‰«æèƒœåˆ©çŠ¶æ€çš„ charsï¼Œæ‰¾å‡º NODE_TO_CHAR é‡Œæœ‰å¯¹åº” glyph ä½†
        å½“å‰ä¸åœ¨ KG è·¯å¾„ä¸­çš„èŠ‚ç‚¹ã€‚å°†å…¶ä½œä¸ºæ–°èŠ‚ç‚¹è¿½åŠ åˆ°è·¯å¾„æœ«å°¾ï¼Œ
        å¹¶ä»¥ 'discovered' relation è¿æ¥åˆ°å‰ä¸€ä¸ªå­ç›®æ ‡ã€‚

        æ³¨æ„ï¼šagent èƒœåˆ©æ—¶é€šå¸¸ç«™åœ¨ç›®æ ‡èŠ‚ç‚¹ï¼ˆæ¥¼æ¢¯ '>'ï¼‰ä¸Šï¼Œ
        æ­¤æ—¶ chars æ˜¾ç¤º '@' è€Œé '>'ï¼Œä¼šå¯¼è‡´æ‰«æå¤±è´¥ã€‚
        å› æ­¤åŒæ—¶æ‰«æ pre_victory_obsï¼ˆèƒœåˆ© option å¼€å§‹å‰çš„è§‚æµ‹ï¼‰ï¼Œ
        å½¼æ—¶ agent å°šæœªç«™ä¸Šç›®æ ‡èŠ‚ç‚¹ï¼Œglyph å¯è§ã€‚
        """
        # ä»ä¸‰ä¸ªæ¥æºåˆå¹¶å€™é€‰èŠ‚ç‚¹ï¼Œè·¨å€™é€‰å»é‡ï¼š
        #   1. episode å†…ä»»æ„æ—¶åˆ»è§è¿‡çš„èŠ‚ç‚¹ï¼ˆä¸»è¦æ¥æºï¼Œç¿»æ—§è´¦ï¼‰
        #   2. pre_victory_obsï¼šwinning option å¼€å§‹å‰ä¸€å¸§ï¼ˆagent æœªç«™ä¸Šç›®æ ‡èŠ‚ç‚¹ï¼‰
        #   3. victory_obsï¼šterminal å¸§ï¼ˆç›®æ ‡èŠ‚ç‚¹å¯èƒ½è¢« @ é®æŒ¡ï¼Œä½œä¸ºå…œåº•ï¼‰
        found_nodes: dict = {n: True for n in self._nodes_seen_this_episode}

        path_nodes = {t for _, _, t in self.path}
        last_node  = self.path[-1][2] if self.path else 'agent'

        for obs in filter(None, [pre_victory_obs, victory_obs]):
            chars = obs.get('chars')
            if chars is None:
                continue
            for node_name, char_set in NODE_TO_CHAR.items():
                if node_name not in found_nodes:
                    if any(int(np.sum(chars == c)) > 0 for c in char_set):
                        found_nodes[node_name] = True

        added = False
        for node_name in found_nodes:
            if node_name in path_nodes:
                continue   # å·²åœ¨è·¯å¾„ä¸­ï¼Œè·³è¿‡
            new_edge = (last_node, 'discovered', node_name)
            self.path.append(new_edge)
            self.node_probs[node_name] = 0.5   # åˆå§‹ä¸­æ€§æ¦‚ç‡ï¼ŒEMA ä¼šè‡ªåŠ¨è°ƒæ•´
            last_node = node_name              # æ”¯æŒé“¾å¼å‘ç°
            path_nodes.add(node_name)
            added = True
            print(
                f"  [KGğŸ”] Discovered: {new_edge[0]} -[discovered]-> {node_name}"
                f"  (glyph visible in pre-victory observation)"
            )

        if added:
            # æ‰©å……è·¯å¾„åé‡å»º bias cacheï¼Œæ–°èŠ‚ç‚¹çº³å…¥è§„åˆ’
            self._bias_cache = [
                _bias_for_path_segment(self.path[i:], self.node_probs, self.options)
                for i in range(len(self.path) + 1)
            ]

    def end_episode(self, success: bool):
        """
        æ¯ä¸ª episode ç»“æŸæ—¶è°ƒç”¨ã€‚
        ç”¨æœ¬è½®ç»éªŒå¯¹ node_probs åš EMA è‡ªæ›´æ–°ï¼Œæ— éœ€äººå·¥æŒ‡å¯¼ã€‚

        æ›´æ–°è§„åˆ™ï¼ˆEMAï¼‰:
          target = 1.0  â†’ è¯¥è¾¹è¢«ç©¿è¶Š ä¸” episode æˆåŠŸï¼ˆæ­£å‘å¼ºåŒ–ï¼‰
          target = 0.0  â†’ å…¶ä»–æƒ…å†µï¼ˆæ¸©å’Œå‹åˆ¶ï¼‰

        å«ä¹‰ï¼šæ¯æ¡è¾¹çš„ prob æ”¶æ•›åˆ°ã€Œç©¿è¶Šè¯¥è¾¹ + æˆåŠŸã€çš„ç»éªŒé¢‘ç‡ã€‚
        """
        # èƒœåˆ©å›æº¯ï¼šæˆåŠŸæ—¶å°†æ‰€æœ‰æœªç©¿è¶Šè·¯å¾„æ­¥éª¤æ ‡è®°ä¸ºå·²ç©¿è¶Šï¼ˆç¡®ä¿å…¨è·¯å¾„è·å¾—æ­£å‘ EMA æ›´æ–°ï¼‰
        if success:
            for i in range(len(self.path)):
                if i not in self._episode_traversed:
                    self._episode_traversed.append(i)

        lr = self.self_update_lr
        for i, (src, rel, dst) in enumerate(self.path):
            if dst not in self.node_probs:
                continue
            traversed = i in self._episode_traversed
            target    = 1.0 if (traversed and success) else 0.0
            old_p     = self.node_probs[dst]
            new_p     = (1 - lr) * old_p + lr * target
            self.node_probs[dst] = float(np.clip(new_p, 0.15, 0.99))

        self._episode_traversed = []
        self.current_step       = 0
        self._total_episodes   += 1

        # Recompute bias cache periodically so option weights reflect learned probs
        if self._total_episodes % self._update_interval == 0:
            self._recompute_bias_cache()
            prob_str = ", ".join(f"{k}:{v:.3f}" for k, v in self.node_probs.items())
            print(f"  [KGâœ¦] Self-updated (ep {self._total_episodes}): {prob_str}")

    def _recompute_bias_cache(self):
        """Rebuild bias vectors from current (self-updated) node_probs."""
        self._bias_cache = [
            _bias_for_path_segment(self.path[i:], self.node_probs, self.options)
            for i in range(len(self.path) + 1)
        ]

    def save_updated_probs(self, path='data/kg_learned_probs.json'):
        """Persist learned node probabilities for inspection / warm-start."""
        os.makedirs(os.path.dirname(os.path.abspath(path)), exist_ok=True)
        with open(path, 'w') as f:
            json.dump({
                'node_probs':      self.node_probs,
                'total_episodes':  self._total_episodes,
            }, f, indent=2)
        print(f"  [KG] Saved learned probs â†’ {path}")

    # â”€â”€ å†…éƒ¨ï¼šglyph æ•°é‡å‡å°‘æ£€æµ‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _glyph_decreased(self, pre_obs, post_obs, dst_node):
        """
        æ£€æŸ¥ dst_node å¯¹åº”çš„ glyph å­—ç¬¦åœ¨åœ°å›¾ä¸Šçš„æ•°é‡æ˜¯å¦å‡å°‘ã€‚
        ä½¿ç”¨ NODE_TO_CHAR æ˜ å°„ï¼ˆKG èŠ‚ç‚¹ â†’ ç¯å¢ƒ glyphï¼Œä¸¤è€…æ„å»ºæ—¶ä¸€è‡´ï¼‰ã€‚
        """
        target_chars = NODE_TO_CHAR.get(dst_node)
        if not target_chars:
            return False

        pre_chars  = pre_obs.get('chars')
        post_chars = post_obs.get('chars')
        if pre_chars is None or post_chars is None:
            return False

        pre_count  = sum(int(np.sum(pre_chars  == c)) for c in target_chars)
        post_count = sum(int(np.sum(post_chars == c)) for c in target_chars)
        return post_count < pre_count


# â”€â”€ å…¼å®¹æ—§æ¥å£ï¼ˆè®­ç»ƒè„šæœ¬åˆå§‹åŒ–æ—¶è°ƒç”¨ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def make_kg_path_state(graph, options, env_name, eta=0.5, start='agent', goal='open'):
    """
    å·¥å‚å‡½æ•°ï¼šä»å·²åŠ è½½çš„ graph æ„å»º KGPathStateã€‚
    è¿”å› (kg_path_state, initial_bias_tensor)ã€‚
    å¦‚æœä¸æ˜¯ KeyRoom ç¯å¢ƒï¼Œè¿”å› (None, zeros)ã€‚
    """
    if 'KeyRoom' not in env_name:
        return None, torch.zeros(len(options))

    node_probs, path = get_node_probs(graph, start, goal)
    if not path:
        return None, torch.zeros(len(options))

    state         = KGPathState(path, node_probs, options, eta=eta)
    initial_bias  = state.get_bias(kg_decay=1.0)
    print(f"[KG] Loaded knowledge graph prior: {initial_bias.tolist()}")
    print(f"[KG] Sub-goal node probs: { {k: f'{v:.4f}' for k, v in node_probs.items()} }")
    return state, initial_bias


if __name__ == '__main__':
    graph = load_graph()

    class FakeOpt:
        def __init__(self, name): self.name = name

    options = [FakeOpt('Explore'), FakeOpt('GoToStairs'), FakeOpt('PickupItem'),
               FakeOpt('FindKey'), FakeOpt('OpenDoor')]

    state, init_bias = make_kg_path_state(graph, options, 'MiniHack-KeyRoom-S5-v0')

    print(f"\n[KG] Initial bias (step 0 â€” no sub-goals completed):")
    for opt, w in zip(options, init_bias):
        print(f"  {opt.name:20s}: {w:+.4f}")

    if state:
        print(f"\n[KG] Bias after key obtained (step 1):")
        state.current_step = 1
        b1 = state.get_bias()
        for opt, w in zip(options, b1):
            print(f"  {opt.name:20s}: {w:+.4f}")

        print(f"\n[KG] Bias after door opened (step 2):")
        state.current_step = 2
        b2 = state.get_bias()
        for opt, w in zip(options, b2):
            print(f"  {opt.name:20s}: {w:+.4f}")
